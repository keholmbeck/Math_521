\input{template.tex}

%----------------------------------------------------------------------------------------
%   NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Homework\ 4} % Assignment title
\newcommand{\hmwkDueDate}{Tuesday,\ April\ 17,\ 2018} % Due date
\newcommand{\hmwkClass}{Math\ 521} % Course/clas
\newcommand{\hmwkAuthorName}{Kristin Holmbeck} % Your name

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%----------------------------------------------------------------------------------------
%   TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\textmd{\textbf{\hmwkClass \ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}
\vspace{0.2in}
}

\author{\textbf{\hmwkAuthorName}}
\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%   TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC
\vspace{0.75in}
\tableofcontents
\listoffigures
\newpage

%----------------------------------------------------------------------------------------
%   PROBLEM 1
%----------------------------------------------------------------------------------------

% To have just one problem per page, simply put a \clearpage after each problem

\begin{section}{Theory}

\begin{homeworkSection}{1. Eigen-relationship}

Consider the two eigenvector problems
\begin{align*}
	C_x \bm{u} = \lambda_x \bm{u}
\end{align*}
and
\begin{align*}
	C_s \bm{v} = \lambda_s \bm{v}
\end{align*}
where the matrices are related by $C_x = C_s + \alpha I$, where $\alpha$ is a real number and $I$ is the usual identity matrix. Show that if $\bm{u}$ is an eigenvector of $C_x$, then it is also an eigenvector of $C_s$ associated with eigenvalue $\lambda_s = \lambda_x -\alpha$.
\\
\\
\problemAnswer{
	\begin{align*}
		C_x \bm{u} &= \lambda_x \bm{u} = C_s \bm{u} + \alpha I \bm{u} \\
		C_s \bm{u} &= \lambda_x \bm{u} - \alpha I \bm{u} \\
		C_s \bm{u} &= (\lambda_x  - \alpha) \bm{u}  \\
		\text{but} \quad C_s \bm{v} &= \lambda_s \bm{v} \\
		\implies \lambda_s &= \lambda_x  - \alpha \qquad \text{is an eigenvalue of $C_s$ with associated eigenvector $\bm{u}$}
	\end{align*}
}

\end{homeworkSection}

\begin{homeworkSection}{2. Invertibility of a particular symmetric matrix}
Let $A \in \mathbb{R}^{m \times n}$. Show that the matrix $M$ defined as
\begin{align*}
	M &= \alpha^2 I + AA^T, \quad  \alpha \neq 0 \in \mathbb{R}
\end{align*}

is nonsingular, where $I = I_m$ and $\alpha$ is a nonzero real number.
\\
\\
\problemAnswer{ 
    \textbf{ THIS IS WRONG. TRY AGAIN }
    \\
    If $M$ is singular, then it has at least one eigenvalue of 0. Since $AA^T$ is symmetric, the diagonal entries
    are non-negative, that is, $(AA^T)_{i,i} = (a_{i,i})^2\geq 0, \forall i = 1, \ldots n$. 
    Examining the trace of the matrix $M$, 
    $$
        tr(M) = \sum{i=1}^n M_{i,i} = \sum{i=1}^n \left ( \alpha^2 + (AA^A)_{i,i} \right ) \geq n \alpha^2 > 0
    $$
    Since the trace of a matrix is the sum of its eigenvalues, (\small{$tr(A) = \sum_{i=1}^n \lambda_i$}), 
    $tr(M)>0$ implies $0$ is not an eigenvalue of $M$. Hence, $M$ is nonsingular.
}

\end{homeworkSection}

\begin{homeworkSection}{3. Between-Class Scatter Matrix simplification}
Show that the between-class scatter matrix, $S_B$, in the multi-class \textit{Fisher Discriminant Analysis} is given by
\begin{align*}
	S_B &= \sum_{i=1}^M n_i (\bm{m}_i - \bm{m})(\bm{m}_i - \bm{m})^T,
\end{align*}
where $M$ is the total number of distinct classes, $n_i$ is the number of data points in class $i$, $\bm{m}_i$ is the class mean of the $i^{\text{th}}$ class, and $m$ is the mean across all $n$ data points. You may use the facts that
\begin{align*}
	S_T = S_B + S_W, \quad S_W = \sum_{i=1}^M \sum_{x \in D_i} (x - \bm{m}_i)(x - \bm{m}_i)^T, \quad \text{and} \quad S_T = \sum_{i=1}^n (x_i - \bm{m})(x_i - \bm{m})^T
\end{align*}
\\
\\
\problemAnswer{
	Given what we know above, and the fact that the i$^{th}$ class mean $\bm{m}_i = \frac{1}{n_i} \sum_{x \in D_i} \bm{x}$ and the mean $\bm{m}$ across all data points is given by $\bm{m} = \frac{1}{n} \sum_{i=1}^n \bm{x}_i$, we will start with evaluating $S_B = S_T - S_W$.
	\\
	\\
	Furthermore, note that $\forall \bm{x}_i \in X$ (the entire data set), $\bm{x}_i \in \bigcup\limits_{j=1}^{M} D_j$, the union of all distinct classes, and also $\bm{m}_i n_i =  \sum_{x \in D_i} \bm{x}$.
	\\
	\begin{align*}
		S_T - S_W &= \sum_{i=1}^n (x_i - \bm{m})(x_i - \bm{m})^T - \sum_{i=1}^M \sum_{x \in D_i} (x - \bm{m}_i)(x - \bm{m}_i)^T \\
		&= \sum_{i=1}^M \sum_{x \in D_i} \left [ (x - \bm{m})(x - \bm{m})^T - (x - \bm{m}_i)(x - \bm{m}_i)^T \right ] \\
		&= \sum_{i=1}^M \sum_{x \in D_i} \left [ (x x^T - \bm{m} x^T -x \bm{m}^T + \bm{m}\bm{m}^T )- (xx^T - \bm{m}_i x^T - x {\bm{m}_i}^T + \bm{m}_i {\bm{m}_i}^T) \right ] \\
		&= \sum_{i=1}^M \sum_{x \in D_i} \left [ (\bm{m}\bm{m}^T - \bm{m} x^T -x \bm{m}^T )- (\bm{m}_i {\bm{m}_i}^T - \bm{m}_i x^T - x {\bm{m}_i}^T) \right ] \\
		&= \sum_{i=1}^M \left [ n_i \bm{m}\bm{m}^T - n_i \bm{m}_i {\bm{m}_i}^T + \sum_{x \in D_i}  (\bm{m}_i x^T + x {\bm{m}_i}^T - \bm{m} x^T - x \bm{m}^T ) \right ] \\
		&= \sum_{i=1}^M \left [ n_i \bm{m}\bm{m}^T - n_i \bm{m}_i {\bm{m}_i}^T + \bm{m}_i \sum_{x \in D_i} x^T + {\bm{m}_i}^T \sum_{x \in D_i} x - \bm{m} \sum_{x \in D_i} x^T - \bm{m}^T \sum_{x \in D_i} x \right ] \\
		&= \sum_{i=1}^M \left [ n_i \bm{m}\bm{m}^T - n_i \bm{m}_i {\bm{m}_i}^T + \bm{m}_i n_i {\bm{m}_i}^T + {\bm{m}_i}^T n_i {\bm{m}_i} - \bm{m} n_i {\bm{m}_i}^T - \bm{m}^T n_i {\bm{m}_i} \right ] \\
		&= \sum_{i=1}^M n_i \left [ \bm{m}\bm{m}^T + {\bm{m}_i}^T {\bm{m}_i} - \bm{m} {\bm{m}_i}^T - \bm{m}^T {\bm{m}_i} \right ] \\
		&= \sum_{i=1}^M n_i (\bm{m} - \bm{m}_i)(\bm{m} - \bm{m}_i)^T
		\\
	\end{align*}
}
\end{homeworkSection}

\end{section}

%----------------------------------------------------------------------------------------
\begin{section}{Computing}

\begin{homeworkSection}{1. KL Procedure for Gappy Data}
This project concerns the application of the KL procedure for incomplete data \cite{everson}. Let the complete data set be translation-invariant:
\begin{align*}
	f(x_m, t_\mu) &= \frac{1}{N} \sum_{k=1}^N \frac{1}{k} \sin[k(x_m-  t_\mu)],
\end{align*}
where $m = 1,\ldots, M$, with $M$ dimension of the ambient space (size of the spatial grid), and $\mu = 1,\ldots,P$, with $P$ the number of points in the ensemble. 
\\
\\
Let $x_m = \cfrac{2\pi(m-1)}{M}$ and $t_\mu = \cfrac{2\pi(\mu-1)}{P}$.
\\
\\
Select an ensemble of masks $\lbrace \bm{m}^{(\mu)} \rbrace$, $\mu = 1, \ldots, P$. where 10\% of the indices are selected to be zero for each mask. Each pattern in the incomplete ensemble may be written as
\begin{align*}
	\tilde{\bm{x}}^{(\mu)} &= \bm{m}^{(\mu)} . \bm{f}^{(\mu)},
\end{align*}
where $\left ( \bm{f}^{(\mu)} \right )_m = \frac{1}{N} \sum_{k=1}^N \frac{1}{k} \sin[k(x_m-  t_\mu)]$. Let $P=M=64$ and $N=3$.

\renewcommand{\theenumi}{\alph{enumi}}
\begin{enumerate}
  \item Compute the eigenvectors of this ensemble using the gappy algorithm \cite{everson}.
  \item Plot the eigenvalues as a function of the iteration, and continue until they converge.
  \item Plot your final eigenfunctions corresponding to the 10 largest eigenvalues.
  \item Plot the element $\tilde{\bm{x}}^{(1)}$ and the vector $\tilde{\bm{x}}_D$ repaired 	according to Equation
  \begin{align}
  	\tilde{\bm{x}} \approx \tilde{\bm{x}}_D = \sum_{n=1}^D \tilde{a}_n \phi^{(n)}.
  \end{align}
  Determine the value of $D$ that provides the best approximation to the original non-gappy pattern vector.
\end{enumerate}

\end{homeworkSection}

\begin{homeworkSection}{2. Linear Discriminant Analysis (LDA)}
\renewcommand{\theenumi}{\alph{enumi}}
\begin{enumerate}
  \item Write a \textsc{MATLAB} routine to produce an optimal projection direction, $w$, using the two-class LDA
    criterion
    $$
        w = \argmax_w J(w) = \argmax_w \frac{w^T S_B w}{w^T S_W w},
    $$
    where
    $$
        S_B = (\bm{m}_2 - \bm{m}_1)(\bm{m}_2 - \bm{m}_1)^T \quad \text{and} \quad
        S_W = \sum_{i=1}^M \sum_{x \in D_i} (x - \bm{m}_i)(x - \bm{m}_i)^T
    $$
    are the between-class scatter matrix and the within-class scatter matrix, respectively. That is, your
    code should take in a set of data points with a clear indication which points belong to class 1 and
    which points belong to class 2, and output a single vector $w$ that is the solution of the generalized
    eigenvalue problem $S_B w = \lambda S_W w$.
  \item Now, use your subroutine in part (a) to project the EEG data onto a real line. Particularly, we can
    form a data point in $\mathbb{R}^{1040×19}$ by concatenating the columns for each trial, therefore having 10 data
    points for task 2 and 10 data points for task 3. You would then project these 20 points onto the
    real line with the $w$ found with part (a). Plot the projected data on the real line and distinguish the
    classes with different symbols. Do you see a clear separation? Analyze your results.
\end{enumerate}
\end{homeworkSection}

\begin{homeworkSection}{3. Maximum Noise Fraction (MNF) method}
Construct a $n \times 10$ matrix (choose $n \geq 250$) to serve as a ground truth data set so that each column is a
$n$-dimensional time series. Next, add \textbf{correlated noise} to each column to create a noisy data set, $X$. The
goal of this problem is to implement the Maximum Noise Fraction method to recover the ground truth as
closely as possible from the noisy data. Suppose the source of the noise is unknown, you may estimate
the noise covariance, $N^TN$, using the difference matrix as $N^T N = \frac{1}{2} dX^TdX$ where if
$$
    X = \begin{bmatrix}
        x_1(t_1)  &&  x_2(t_1) && \cdots && x_p(t_1) \\
        x_1(t_2)  &&  x_2(t_2) && \cdots && x_p(t_2) \\
        \vdots    && \vdots    && \ddots && \vdots \\
        x_1(t_n)  &&  x_2(t_n) && \cdots && x_p(t_n)
    \end{bmatrix}
$$
then
$$
    dX = \begin{bmatrix}
        x_1(t_2)-x_1(t_1)  &&  x_2(t_2)-x_2(t_1) && \cdots && x_2(t_2) - x_p(t_1) \\
        x_1(t_3)-x_1(t_2)  &&  x_2(t_3)-x_2(t_2) && \cdots && x_2(t_3) - x_p(t_2) \\
        \vdots    && \vdots    && \ddots && \vdots \\
        x_1(t_n)-x_1(t_{n-1})  &&  x_2(t_n)-x_2(t_{n-1}) && \cdots && x_2(t_n) - x_p(t_{n-1})
    \end{bmatrix}
$$
Notice that $X \in \mathbb{R}^{n \times p}$ and $dX \in \mathbb{R}^{(n-1) \times p}$.
In your report, examine and elaborate on the effect of a $D$-mode
reconstruction on a single noisy signal for various values of $D$ (i.e., choose a single column to filter). In
a single graph, visually display the result of the original signal, noisy signal, and filtered (de-noised) data
(with your best choice of $D$) to compare. Use the graph legend to distinguish each.

\end{homeworkSection}

\end{section}
%----------------------------------------------------------------------------------------
\newpage

\appendix

\section{Code}\label{code}

\subsection{Gram-Schmidt} \label{code:gram_schmidt}

% \lstinputlisting{../Kristin_Holmbeck_HW2_GramSchmidt.m}


\begin{thebibliography}{10}
    \bibitem{chang}
    Chang, Jen-Mei. \textit{Matrix Methods for Geometric Data Analysis and Recognition}. 2014.

    \bibitem{fisher}
    P. N. Belhumeur, J. P. Hespanha and D. J. Kriegman, ``Eigenfaces vs. Fisherfaces: recognition using class specific linear projection," in \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 19, no. 7, pp. 711-720, Jul 1997.

    \bibitem{everson}
    R. Everson and L. Sirovich. The karhunen-loeve transform for incomplete data. \textit{J. Opt. Soc. Am., A}, 12(8):1657–1664, 1995.
\end{thebibliography}

\end{document}
